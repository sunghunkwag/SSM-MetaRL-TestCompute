"""
main.py - Minimal SSM + Meta-RL skeleton with test-time compute hooks

This script provides a compact, educational example that demonstrates:
- A minimal State Space Model (SSM) with linear state transitions
- A meta-RL training loop skeleton (MAML-like) with clear placeholders
- Test-time compute hooks for online adaptation of parameters at inference

Note: This is an experimental, didactic sample. It is not production-ready.
All functions are heavily commented for clarity.
"""

from __future__ import annotations
from dataclasses import dataclass
from typing import Tuple, Dict, Any, Iterable
import numpy as np

# Optional: If extending with PyTorch
try:
    import torch
    from torch import nn, optim
    TORCH_AVAILABLE = True
except Exception:
    TORCH_AVAILABLE = False


# =============================
# Minimal State Space Model (SSM)
# =============================
@dataclass
class SSMConfig:
    state_dim: int = 8
    input_dim: int = 4
    output_dim: int = 2
    dt: float = 1.0  # discrete time step


class MinimalSSM:
    """
    Minimal linear state-space model:
      x_{t+1} = A x_t + B u_t
      y_t     = C x_t
    where x is the latent state, u is the input/observation, and y is the output.
    """

    def __init__(self, cfg: SSMConfig, seed: int = 0):
        rng = np.random.default_rng(seed)
        self.cfg = cfg
        # Randomly initialize small weights for A, B, C
        self.A = 0.1 * rng.standard_normal((cfg.state_dim, cfg.state_dim))
        self.B = 0.1 * rng.standard_normal((cfg.state_dim, cfg.input_dim))
        self.C = 0.1 * rng.standard_normal((cfg.output_dim, cfg.state_dim))
        # Initialize hidden state
        self.h = np.zeros((cfg.state_dim,), dtype=np.float32)

    def reset_state(self):
        self.h[:] = 0.0

    def step(self, u_t: np.ndarray) -> np.ndarray:
        """
        Advance state and produce output.
        u_t: shape [input_dim]
        Returns y_t: shape [output_dim]
        """
        # State transition
        self.h = self.A @ self.h + self.B @ u_t
        # Output mapping
        y = self.C @ self.h
        return y

    def parameters(self) -> Dict[str, np.ndarray]:
        """Return a dict of model parameters for convenience."""
        return {"A": self.A, "B": self.B, "C": self.C}

    def set_parameters(self, params: Dict[str, np.ndarray]):
        """Set parameters from a dict (used for adaptation)."""
        self.A = params["A"]
        self.B = params["B"]
        self.C = params["C"]


# =====================================
# Synthetic Task and Loss (Toy Example)
# =====================================

def toy_task_generator(num_tasks: int, cfg: SSMConfig, seed: int = 42) -> Iterable[Dict[str, Any]]:
    """
    Generate a distribution of tasks. Each task provides a simple linear mapping
    to predict a target y from inputs u via the SSM. In practice, tasks would
    represent different MDPs or environments.
    """
    rng = np.random.default_rng(seed)
    for _ in range(num_tasks):
        # Task-specific target projection (unknown to the learner)
        W_task = 0.5 * rng.standard_normal((cfg.output_dim, cfg.input_dim))
        # Support (adaptation) and query (evaluation) splits
        support = rng.standard_normal((16, cfg.input_dim))
        query = rng.standard_normal((16, cfg.input_dim))
        # Targets generated by unknown task mapping
        y_support = support @ W_task.T
        y_query = query @ W_task.T
        yield {
            "support": (support, y_support),
            "query": (query, y_query),
        }


def mse(pred: np.ndarray, target: np.ndarray) -> float:
    return float(np.mean((pred - target) ** 2))


# =============================
# Meta-RL (MAML-like) Skeleton
# =============================
@dataclass
class MetaConfig:
    inner_steps: int = 1         # adaptation steps per task
    inner_lr: float = 0.1        # learning rate for inner loop
    outer_steps: int = 5         # number of meta-iterations
    tasks_per_batch: int = 4     # tasks per meta-batch


def clone_params(params: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
    return {k: v.copy() for k, v in params.items()}


def ssm_forward_sequence(model: MinimalSSM, x: np.ndarray) -> np.ndarray:
    """
    Run the SSM over a sequence of inputs (x) and collect outputs.
    x: [T, input_dim]
    Returns Y: [T, output_dim]
    """
    model.reset_state()
    outputs = []
    for t in range(x.shape[0]):
        y_t = model.step(x[t])
        outputs.append(y_t)
    return np.stack(outputs, axis=0)


def inner_adaptation(params: Dict[str, np.ndarray], support: Tuple[np.ndarray, np.ndarray], cfg: SSMConfig, inner_lr: float) -> Dict[str, np.ndarray]:
    """
    One gradient-like update step on support set.
    This uses a finite-difference-style pseudo-gradient for illustration only.
    In a real system, use autograd (e.g., PyTorch) and backpropagate through time.
    """
    model = MinimalSSM(cfg)
    model.set_parameters(clone_params(params))

    x_s, y_s = support
    preds = ssm_forward_sequence(model, x_s)

    # Compute simple gradients w.r.t. parameters via numeric approximation
    grads = {k: np.zeros_like(v) for k, v in params.items()}

    def loss_fn() -> float:
        return mse(preds, y_s)

    base_loss = loss_fn()
    eps = 1e-3

    # For brevity, perturb only C (output mapping) in this toy example
    for i in range(model.C.shape[0]):
        for j in range(model.C.shape[1]):
            orig = model.C[i, j]
            model.C[i, j] = orig + eps
            preds_eps = ssm_forward_sequence(model, x_s)
            loss_eps = mse(preds_eps, y_s)
            grads["C"][i, j] = (loss_eps - base_loss) / eps
            model.C[i, j] = orig

    # Gradient descent update on C only (for illustration)
    new_params = clone_params(params)
    new_params["C"] -= inner_lr * grads["C"]
    return new_params


def meta_train(cfg: SSMConfig, mcfg: MetaConfig, seed: int = 123) -> Dict[str, np.ndarray]:
    """
    Outer-loop meta-training that learns initialization parameters which can be
    quickly adapted by inner_adaptation on new tasks.
    """
    rng = np.random.default_rng(seed)
    model = MinimalSSM(cfg, seed=seed)
    params = model.parameters()

    for step in range(mcfg.outer_steps):
        batch_tasks = list(toy_task_generator(mcfg.tasks_per_batch, cfg, seed=rng.integers(1e9)))

        meta_loss_accum = 0.0
        meta_grads = {k: np.zeros_like(v) for k, v in params.items()}

        for task in batch_tasks:
            # Inner adaptation on support set
            adapted_params = inner_adaptation(params, task["support"], cfg, mcfg.inner_lr)

            # Evaluate on query split
            eval_model = MinimalSSM(cfg)
            eval_model.set_parameters(clone_params(adapted_params))
            x_q, y_q = task["query"]
            preds_q = ssm_forward_sequence(eval_model, x_q)
            task_loss = mse(preds_q, y_q)
            meta_loss_accum += task_loss

            # Pseudo-gradient accumulation on C only (toy)
            eps = 1e-3
            for i in range(eval_model.C.shape[0]):
                for j in range(eval_model.C.shape[1]):
                    orig = eval_model.C[i, j]
                    eval_model.C[i, j] = orig + eps
                    preds_eps = ssm_forward_sequence(eval_model, x_q)
                    loss_eps = mse(preds_eps, y_q)
                    meta_grads["C"][i, j] += (loss_eps - task_loss) / eps
                    eval_model.C[i, j] = orig

        # Average gradients and update initialization params (C only here)
        meta_loss = meta_loss_accum / max(1, len(batch_tasks))
        meta_grads["C"] /= max(1, len(batch_tasks))
        params["C"] -= mcfg.inner_lr * meta_grads["C"]

        print(f"[meta] step={step} loss={meta_loss:.6f}")

    return params


# ==============================================
# Test-Time Compute: Online Adaptation Hooks
# ==============================================

def test_time_adapt(params: Dict[str, np.ndarray], obs: np.ndarray, target: np.ndarray, cfg: SSMConfig, lr: float = 0.05) -> Dict[str, np.ndarray]:
    """
    Perform a lightweight online update using a single observation-target pair.
    Here we again adapt only C via a finite-difference pseudo-gradient.
    """
    model = MinimalSSM(cfg)
    model.set_parameters(clone_params(params))
    preds = ssm_forward_sequence(model, obs[None, :])  # shape [1, output_dim]
    base_loss = mse(preds, target[None, :])

    eps = 1e-3
    grad_C = np.zeros_like(model.C)
    for i in range(model.C.shape[0]):
        for j in range(model.C.shape[1]):
            orig = model.C[i, j]
            model.C[i, j] = orig + eps
            preds_eps = ssm_forward_sequence(model, obs[None, :])
            loss_eps = mse(preds_eps, target[None, :])
            grad_C[i, j] = (loss_eps - base_loss) / eps
            model.C[i, j] = orig

    new_params = clone_params(params)
    new_params["C"] -= lr * grad_C
    return new_params


def run_inference_with_test_time_compute(params: Dict[str, np.ndarray], inputs: np.ndarray, cfg: SSMConfig, targets: np.ndarray | None = None, adapt: bool = True) -> np.ndarray:
    """
    Run inference, optionally performing online adaptation using test-time compute.
    If targets are provided, we simulate supervised feedback for adaptation.
    """
    model = MinimalSSM(cfg)
    model.set_parameters(clone_params(params))
    outputs = []

    for t in range(inputs.shape[0]):
        u_t = inputs[t]
        y_t = model.step(u_t)
        outputs.append(y_t)

        # Optional online adaptation if target for this step is known
        if adapt and targets is not None:
            params = test_time_adapt(model.parameters(), u_t, targets[t], cfg)
            model.set_parameters(params)

    return np.stack(outputs, axis=0)


# ========
# Script
# ========

def main():
    cfg = SSMConfig(state_dim=8, input_dim=4, output_dim=2)
    mcfg = MetaConfig(inner_steps=1, inner_lr=0.1, outer_steps=3, tasks_per_batch=3)

    print("Training meta-initialization (toy example)...")
    init_params = meta_train(cfg, mcfg)

    print("\nRunning inference with (simulated) test-time compute...")
    rng = np.random.default_rng(0)
    inputs = rng.standard_normal((10, cfg.input_dim))
    true_W = 0.3 * rng.standard_normal((cfg.output_dim, cfg.input_dim))
    targets = inputs @ true_W.T  # synthetic targets to enable adaptation demo

    outputs = run_inference_with_test_time_compute(init_params, inputs, cfg, targets=targets, adapt=True)
    print("Outputs shape:", outputs.shape)


if __name__ == "__main__":
    main()
